{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KOTE test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"searle-j/kote_for_easygoing_people\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"searle-j/kote_for_easygoing_people\")\n",
    "\n",
    "# 감정 분석 모델 설정 \n",
    "pipe = TextClassificationPipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0, # gpu number, -1 if cpu used\n",
    "        return_all_scores=True,\n",
    "        function_to_apply='sigmoid'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>like_cnt</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOME SWEET HOME (feat. 태양, 대성)</td>\n",
       "      <td>G-DRAGON</td>\n",
       "      <td>2,220</td>\n",
       "      <td>You say Its changed Show must go on Behave 오랜만...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POWER</td>\n",
       "      <td>G-DRAGON</td>\n",
       "      <td>1,792</td>\n",
       "      <td>When GDs in the house bermensch When GDs in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Igloo</td>\n",
       "      <td>KISS OF LIFE</td>\n",
       "      <td>790</td>\n",
       "      <td>Imma back up every word Mini skirt pretty pink...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small girl (feat. 도경수(D.O.))</td>\n",
       "      <td>이영지</td>\n",
       "      <td>3,308</td>\n",
       "      <td>If I got a two small cheeks and a bright pink ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPOT! (feat. JENNIE)</td>\n",
       "      <td>지코 (ZICO)</td>\n",
       "      <td>2,574</td>\n",
       "      <td>Everything ok my man Turn the music up  Uhm we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>연</td>\n",
       "      <td>오반(OVAN)</td>\n",
       "      <td>22</td>\n",
       "      <td>우 어쩌면 우리는 Babe   우리도 모르게 Babe   서로를 계속 원했을까 아마...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>살아간다</td>\n",
       "      <td>최유리</td>\n",
       "      <td>616</td>\n",
       "      <td>나는 조금만 더 올라가면 보일걸  그 말을 굳게 믿은 채 다시 살아간다  나는 조그...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>Smoking Dreams</td>\n",
       "      <td>재지팩트(Jazzyfact)</td>\n",
       "      <td>3,969</td>\n",
       "      <td>i puff onetime and i puff twice   on and on an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>사랑했던 우리는 없어</td>\n",
       "      <td>리</td>\n",
       "      <td>12</td>\n",
       "      <td>뜨겁게 사랑했던 우리는 없어  한없이 행복했던 우리는 없어  노력해도 안되는  답이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>White Snow (Feat. prettyhappy)</td>\n",
       "      <td>Dept(뎁트)</td>\n",
       "      <td>102</td>\n",
       "      <td>Cuz I feel the warmest now that Im with you ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1396 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title           artist like_cnt  \\\n",
       "0     HOME SWEET HOME (feat. 태양, 대성)         G-DRAGON    2,220   \n",
       "1                              POWER         G-DRAGON    1,792   \n",
       "2                              Igloo     KISS OF LIFE      790   \n",
       "3       Small girl (feat. 도경수(D.O.))              이영지    3,308   \n",
       "4               SPOT! (feat. JENNIE)        지코 (ZICO)    2,574   \n",
       "...                              ...              ...      ...   \n",
       "1391                               연         오반(OVAN)       22   \n",
       "1392                            살아간다              최유리      616   \n",
       "1393                  Smoking Dreams  재지팩트(Jazzyfact)    3,969   \n",
       "1394                     사랑했던 우리는 없어                리       12   \n",
       "1395  White Snow (Feat. prettyhappy)         Dept(뎁트)      102   \n",
       "\n",
       "                                                 lyrics  \n",
       "0     You say Its changed Show must go on Behave 오랜만...  \n",
       "1     When GDs in the house bermensch When GDs in th...  \n",
       "2     Imma back up every word Mini skirt pretty pink...  \n",
       "3     If I got a two small cheeks and a bright pink ...  \n",
       "4     Everything ok my man Turn the music up  Uhm we...  \n",
       "...                                                 ...  \n",
       "1391  우 어쩌면 우리는 Babe   우리도 모르게 Babe   서로를 계속 원했을까 아마...  \n",
       "1392  나는 조금만 더 올라가면 보일걸  그 말을 굳게 믿은 채 다시 살아간다  나는 조그...  \n",
       "1393  i puff onetime and i puff twice   on and on an...  \n",
       "1394  뜨겁게 사랑했던 우리는 없어  한없이 행복했던 우리는 없어  노력해도 안되는  답이...  \n",
       "1395  Cuz I feel the warmest now that Im with you ba...  \n",
       "\n",
       "[1396 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"new_merge.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "text = df['lyrics'].tolist()\n",
    "\n",
    "# 긴 텍스트 자르기 함수\n",
    "def truncate_texts(texts, tokenizer, max_length):   # 입력 텍스트 리스트를 받아, 각 텍스트를 최대 길이로 자르고 잘라낸 텍스트를 반환합니다.\n",
    "    truncated_texts = []\n",
    "    for t in texts:\n",
    "        tokens = tokenizer(t, max_length=max_length, truncation=True)       #텍스트 t를 토큰화할 때, 최대 길이 max_length로 잘라내고 필요하면 truncation=True로 설정하여 초과 부분을 제거합니다.\n",
    "        truncated_texts.append(tokenizer.decode(tokens['input_ids'], skip_special_tokens=True))     #잘라낸 토큰을 다시 텍스트로 변환하며, 특수 토큰은 제거합니다.\n",
    "    return truncated_texts\n",
    "\n",
    "# 텍스트 리스트를 잘라서 파이프라인에 전달\n",
    "max_length = tokenizer.model_max_length  # 일반적으로 512\n",
    "truncated_texts = truncate_texts(text, tokenizer, max_length)   # text 리스트의 각 텍스트를 최대 길이로 잘라냅니다.\n",
    "pip_text = pipe(truncated_texts)        #잘라낸 텍스트를 파이프라인에 전달하여 감정 분석을 수행합니다.\n",
    "\n",
    "\n",
    "# 결과 저장을 위한 리스트\n",
    "results = []\n",
    "\n",
    "# 각 감정에 해당할 확률(score)이 0.4 이상인 감정들만 저장\n",
    "for i, t in enumerate(truncated_texts):            # 잘라낸 텍스트와 그에 대한 감정 분석 결과를 반복합니다.\n",
    "    for output in pip_text[i]:\n",
    "        if output[\"score\"] > 0.4:\n",
    "            results.append({\n",
    "                \"text\": t,\n",
    "                \"label\": output[\"label\"],\n",
    "                \"score\": output[\"score\"]\n",
    "            })\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임 저장 \n",
    "results_df.to_csv(\"kote_result.csv\", index=False)\n",
    "\n",
    "# # 각 감정에 해당할 확률(score)이 0.4 이상인 감정들만 출력\n",
    "# for i, t in enumerate(truncated_texts):     # 잘라낸 텍스트와 그에 대한 감정 분석 결과를 반복합니다.\n",
    "#     print(f\"Text: {t}\")\n",
    "#     for output in pip_text[i]:\n",
    "#         if output[\"score\"] > 0.4:\n",
    "#             print(output)\n",
    "#     print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
